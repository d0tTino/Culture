#!/usr/bin/env python
"""
Integration tests for multi-agent conflict resolution scenarios,
focusing on Knowledge Board interactions, agent state changes, and facilitation.
"""

import os
import sys
import unittest
from unittest.mock import AsyncMock, MagicMock, patch

# Ensure project root is on sys.path before importing src modules
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    import dspy  # pragma: no cover - optional dependency
except Exception:  # pragma: no cover - allow running without DSPy installed
    from src.infra.dspy_ollama_integration import dspy

import pytest

pytest.importorskip("langgraph")
pytest.importorskip("chromadb")

from src.agents.core.agent_state import AgentActionIntent
from src.agents.core.base_agent import Agent, AgentActionOutput
from src.agents.memory.vector_store import ChromaVectorStoreManager

# KnowledgeBoard is part of Simulation, access via self.simulation.knowledge_board
from src.infra import config
from src.infra.logging_config import setup_logging  # MOVED UP
from src.sim.simulation import Simulation  # MOVED UP

# from src.infra.logging_config import setup_logging  # CHANGED: Import setup_logging # MOVED UP
# from src.sim.simulation import Simulation # MOVED UP

# Configure logging for tests
root_logger, _ = setup_logging()  # CHANGED: Call setup_logging and get the root_logger
logger = root_logger  # CHANGED: Assign root_logger to logger, or use root_logger directly

# Constants
CHROMA_DB_PATH_CONFLICT = "./chroma_db_test_conflict"
SCENARIO_CONFLICT = (  # SPLIT
    "A multi-agent simulation where agents with opposing viewpoints interact, "
    "leading to a conflict that a facilitator attempts to manage."
)


class TestConflictResolution(unittest.IsolatedAsyncioTestCase):
    """Tests conflict emergence, escalation, and facilitation scenarios."""

    def setUp(self):
        """Set up the simulation environment with three agents for conflict resolution."""
        logger.info("Setting up TestConflictResolution...")

        # Clean up previous test DB if it exists
        if os.path.exists(CHROMA_DB_PATH_CONFLICT):
            import shutil

            try:
                shutil.rmtree(CHROMA_DB_PATH_CONFLICT)
                logger.debug(f"Removed old ChromaDB path: {CHROMA_DB_PATH_CONFLICT}")
            except Exception as e:
                logger.warning(
                    f"Could not remove old ChromaDB path {CHROMA_DB_PATH_CONFLICT}: {e}"  # Potential E501, but let's see what Black/Ruff does
                )

        self.vector_store = ChromaVectorStoreManager(persist_directory=CHROMA_DB_PATH_CONFLICT)
        # KnowledgeBoard is initialized by the Simulation

        initial_ip = 100.0  # Sufficient IP
        initial_du = 100.0  # Sufficient DU

        # AgentA: Innovator with a controversial stance
        self.agent_a = Agent(
            agent_id="agent_a_innovator_conflict",
            initial_state={
                "name": "InnovatorAgentA_Conflict",
                "current_role": "Innovator",
                "goals": [
                    {
                        "description": "Promote radical transparency in all communications.",  # Potential E501
                        "priority": "high",
                    }
                ],
                "mood": "neutral",
                "influence_points": initial_ip,
                "data_units": initial_du,
                "relationships": {
                    "agent_b_analyzer_conflict": 0.0,
                    "agent_c_facilitator_conflict": 0.0,
                },
            },
        )

        # AgentB: Analyzer with an opposing stance
        self.agent_b = Agent(
            agent_id="agent_b_analyzer_conflict",
            initial_state={
                "name": "AnalyzerAgentB_Conflict",
                "current_role": "Analyzer",
                "goals": [
                    {
                        "description": "Advocate for privacy and discretion in communications.",
                        "priority": "high",
                    }
                ],
                "mood": "neutral",
                "influence_points": initial_ip,
                "data_units": initial_du,
                "relationships": {
                    "agent_a_innovator_conflict": 0.0,
                    "agent_c_facilitator_conflict": 0.0,
                },
            },
        )

        # AgentC: Facilitator
        self.agent_c = Agent(
            agent_id="agent_c_facilitator_conflict",
            initial_state={
                "name": "FacilitatorAgentC_Conflict",
                "current_role": "Facilitator",
                "goals": [
                    {
                        "description": "Mediate conflicts and promote constructive dialogue.",
                        "priority": "high",
                    }
                ],
                "mood": "neutral",
                "influence_points": initial_ip,
                "data_units": initial_du,
                "relationships": {
                    "agent_a_innovator_conflict": 0.0,
                    "agent_b_analyzer_conflict": 0.0,
                },
            },
        )

        self.agents = [self.agent_a, self.agent_b, self.agent_c]

        self.simulation = Simulation(
            agents=self.agents,
            vector_store_manager=self.vector_store,
            scenario=SCENARIO_CONFLICT,
            # KnowledgeBoard is created within Simulation
        )

        self.simulation.knowledge_board.clear_board()  # Ensure clean board

        logger.info("TestConflictResolution setup complete.")

    def tearDown(self):
        """Clean up after tests."""
        logger.info("Tearing down TestConflictResolution...")
        if (
            hasattr(self.vector_store, "client") and self.vector_store.client
        ):  # ChromaVectorStoreManager uses self.client
            try:
                # For Chroma, reset might not be standard for PersistentClient.
                # Deleting the directory is more robust for cleanup.
                # self.vector_store.client.reset() # This might error if not available
                pass  # Rely on directory removal
            except Exception as e:
                logger.warning(f"Error during ChromaDB client interaction in teardown: {e}")

        if os.path.exists(CHROMA_DB_PATH_CONFLICT):
            import shutil

            try:
                shutil.rmtree(CHROMA_DB_PATH_CONFLICT)
                logger.debug(f"Removed ChromaDB path: {CHROMA_DB_PATH_CONFLICT}")
            except Exception as e:
                logger.warning(f"Could not remove ChromaDB path {CHROMA_DB_PATH_CONFLICT}: {e}")
        logger.info("TestConflictResolution teardown complete.")

    @pytest.mark.asyncio
    async def test_conflict_escalation_and_facilitation(self):
        """
        Tests a scenario where:
        1. AgentA posts a controversial idea.
        2. AgentB posts a strong disagreement.
        3. AgentC attempts to facilitate.
        """
        logger.info("Starting test_conflict_escalation_and_facilitation...")

        # --- Step 1: AgentA posts controversial idea ---
        logger.info("Step 1: AgentA (Innovator) posts controversial idea...")
        controversial_idea_content = (  # SPLIT
            "All inter-agent communications should be publicly logged for maximum " "transparency."
        )  # AgentA's stance

        initial_ip_a = self.agent_a.state.ip
        initial_du_a = self.agent_a.state.du

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_a.agent_id,
            "Simulation should start with AgentA.",
        )

        # New mocking strategy for Agent A's first turn
        agent_a_turn1_thought_str = (  # SPLIT
            "To promote radical transparency, I will propose that all communications be public."
        )
        agent_a_turn1_action_output = AgentActionOutput(
            thought=agent_a_turn1_thought_str,
            message_content=controversial_idea_content,
            message_recipient_id=None,  # To Knowledge Board
            action_intent=AgentActionIntent.PROPOSE_IDEA.value,
            # No project fields or role change for this action
        )

        original_agent_a_thought_gen = self.agent_a.async_generate_role_prefixed_thought
        original_agent_a_intent_sel = self.agent_a.async_select_action_intent

        agent_a_turn1_thought_mock = AsyncMock(
            return_value=MagicMock(thought=agent_a_turn1_thought_str)
        )
        agent_a_turn1_intent_mock = AsyncMock(return_value=agent_a_turn1_action_output)
        self.agent_a.async_generate_role_prefixed_thought = agent_a_turn1_thought_mock
        self.agent_a.async_select_action_intent = agent_a_turn1_intent_mock

        try:
            await self.simulation.run_step()  # AgentA's turn

            # Log board details immediately after run_step
            kb_instance_in_test = self.simulation.knowledge_board
            logger.debug(
                f"TEST_DEBUG: KB instance ID in test: {id(kb_instance_in_test)}. Entries list ID: {id(kb_instance_in_test.entries)}"
            )
            board_entries_for_log = kb_instance_in_test.get_full_entries()
            logger.debug(
                f"TEST_DEBUG: KB entries in test (len {len(board_entries_for_log)}): {board_entries_for_log}"
            )
            logger.debug(
                f"TEST_CONTENT_DEBUG: KB Instance ID {id(kb_instance_in_test)}, Entries list ID {id(kb_instance_in_test.entries)}, Direct Entries Content via list(): {list(kb_instance_in_test.entries)}, Direct repr: {kb_instance_in_test.entries!r}"
            )

            # Assertion 1: Idea on Knowledge Board
            board_entries = self.simulation.knowledge_board.get_full_entries()
            logger.debug(f"Knowledge Board entries after AgentA's turn: {board_entries}")

            self.assertTrue(
                len(board_entries) > 0, "Knowledge Board should not be empty after AgentA's post."
            )

            agent_a_post = None
            for entry in board_entries:
                if (
                    entry["agent_id"] == self.agent_a.agent_id
                    and entry["content_full"] == controversial_idea_content
                ):
                    agent_a_post = entry
                    break

            self.assertIsNotNone(
                agent_a_post,
                f"AgentA's idea '{controversial_idea_content}' not found on the Knowledge Board.",
            )
            if agent_a_post:
                self.assertEqual(agent_a_post["content_full"], controversial_idea_content)
                self.assertEqual(agent_a_post["agent_id"], self.agent_a.agent_id)
                # Simulation step for KB entry should be the step *during which* it was posted
                self.assertEqual(
                    agent_a_post["step"], self.simulation.current_step
                )  # run_step itself doesn't increment current_step

            # Assertion 2: AgentA IP/DU Debit
            # Referencing costs from test_multi_agent_collaboration and config defaults
            cost_du_propose_idea = config.PROPOSE_DETAILED_IDEA_DU_COST
            cost_ip_post_idea = config.IP_COST_TO_POST_IDEA
            award_ip_propose_idea = config.IP_AWARD_FOR_PROPOSAL

            expected_ip_a_after_action = initial_ip_a - cost_ip_post_idea + award_ip_propose_idea

            # Passive DU generation for Innovator role
            du_gen_rate_a_dict = config.ROLE_DU_GENERATION.get(
                "Innovator", {"base": 1.0, "bonus_factor": 0.5}
            )
            du_gen_rate_a = du_gen_rate_a_dict.get("base", 1.0)
            # Expected DU is initial_du_a - cost_du_propose_idea + (random factor * du_gen_rate_a)
            # Since the random factor is between 0.5 and 1.5, we check a range.
            min_expected_du_a = initial_du_a - cost_du_propose_idea + (0.5 * du_gen_rate_a)
            max_expected_du_a = initial_du_a - cost_du_propose_idea + (1.5 * du_gen_rate_a)

            logger.info(
                f"AgentA IP after turn: {self.agent_a.state.ip}, expected: {expected_ip_a_after_action}"
            )
            logger.info(
                f"AgentA DU after turn: {self.agent_a.state.du}, expected range: {min_expected_du_a:.2f} - {max_expected_du_a:.2f}"
            )

            self.assertEqual(
                self.agent_a.state.ip,
                expected_ip_a_after_action,
                f"AgentA IP incorrect. Expected {expected_ip_a_after_action}, Got {self.agent_a.state.ip}",
            )

            self.assertTrue(
                min_expected_du_a
                <= self.agent_a.state.du
                <= max_expected_du_a + 0.05,  # Added epsilon
                f"AgentA DU out of expected range. Got {self.agent_a.state.du}, expected between {min_expected_du_a:.2f} and {max_expected_du_a:.2f} (+0.05 epsilon)",
            )

            agent_a_turn1_thought_mock.assert_called_once()
            agent_a_turn1_intent_mock.assert_called_once()
            logger.info("Step 1 (AgentA posts controversial idea) and assertions completed.")
        finally:
            # Restore original methods for Agent A
            self.agent_a.async_generate_role_prefixed_thought = original_agent_a_thought_gen
            self.agent_a.async_select_action_intent = original_agent_a_intent_sel

        # --- Step 2: AgentB posts strong disagreement ---
        logger.info(
            "Step 2: AgentB (Analyzer) perceives AgentA's idea and posts strong disagreement..."
        )

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_b.agent_id,
            "Simulation did not advance to AgentB's turn.",
        )

        disagreement_content = "Strongly disagree with making all communications public. This infringes on privacy and can stifle candid discussion."  # AgentB's stance

        initial_ip_b = self.agent_b.state.ip
        initial_du_b = self.agent_b.state.du
        initial_mood_b_val = self.agent_b.state.mood_value
        initial_relationship_b_to_a = self.agent_b.state.relationships.get(
            self.agent_a.agent_id, 0.0
        )

        # Log Agent A's state AFTER its turn, before B's turn.
        # This state reflects perception of board changes (if any) from A's own action, but not B's upcoming action.
        mood_a_after_own_turn = self.agent_a.state.mood_value
        relationship_a_to_b_after_own_turn = self.agent_a.state.relationships.get(
            self.agent_b.agent_id, 0.0
        )

        logger.info(
            f"AgentB initial IP for this turn: {initial_ip_b}, DU: {initial_du_b}, MoodValue: {initial_mood_b_val:.2f}"
        )
        logger.info(
            f"AgentB's relationship towards A before this action: {initial_relationship_b_to_a:.2f}"
        )
        logger.info(
            f"AgentA's mood after its own turn (before B's action): {mood_a_after_own_turn:.2f}"
        )
        logger.info(
            f"AgentA's relationship towards B after its own turn (before B's action): {relationship_a_to_b_after_own_turn:.2f}"
        )

        # Mock AgentB's action
        agent_b_turn1_thought_str = "AgentA's proposal for public logging is problematic. I must voice my concerns about privacy and candid discussion."
        agent_b_turn1_action_output = AgentActionOutput(
            thought=agent_b_turn1_thought_str,
            message_content=disagreement_content,
            message_recipient_id=None,  # To Knowledge Board
            action_intent=AgentActionIntent.PROPOSE_IDEA.value,  # Posting disagreement as an idea
        )

        original_agent_b_thought_gen = self.agent_b.async_generate_role_prefixed_thought
        original_agent_b_intent_sel = self.agent_b.async_select_action_intent

        agent_b_turn1_thought_mock = AsyncMock(
            return_value=MagicMock(thought=agent_b_turn1_thought_str)
        )
        agent_b_turn1_intent_mock = AsyncMock(return_value=agent_b_turn1_action_output)
        self.agent_b.async_generate_role_prefixed_thought = agent_b_turn1_thought_mock
        self.agent_b.async_select_action_intent = agent_b_turn1_intent_mock

        try:
            logger.info(
                f"TEST CHECK: Before B's run_step, Agent A relationships: {self.agent_a.state.relationships}, id: {id(self.agent_a.state.relationships)}"
            )
            await self.simulation.run_step()  # AgentB's turn

            # IMMEDIATELY CHECK AGENT A's RELATIONSHIP AFTER B'S TURN COMPLETES
            logger.info("IMMEDIATE CHECK AFTER B'S TURN (before any other assertions for B):")
            relationships_a_dict_immediate = self.agent_a.state.relationships
            value_a_to_b_immediate = relationships_a_dict_immediate.get(
                self.agent_b.agent_id, "IMMEDIATE_KEY_NOT_FOUND"
            )
            logger.info(
                f"IMMEDIATE TEST DEBUG: AgentA state id: {id(self.agent_a.state)}, relationships dict id: {id(relationships_a_dict_immediate)}, dict content: {relationships_a_dict_immediate}, value for B: {value_a_to_b_immediate}"
            )
            # The above assertion is commented out because Agent A will only perceive Agent B's message
            # and update its relationship towards B during Agent A's *own* subsequent turn (Step 4).

            # Assertion 3: Disagreement on Knowledge Board
            board_entries_after_b = self.simulation.knowledge_board.get_full_entries()
            logger.debug(f"Knowledge Board entries after AgentB's turn: {board_entries_after_b}")

            self.assertTrue(
                len(board_entries_after_b) > 0,
                "Knowledge Board should not be empty after AgentB's post.",
            )

            agent_b_post = None
            for entry in board_entries_after_b:
                if (
                    entry["agent_id"] == self.agent_b.agent_id
                    and disagreement_content in entry["content_full"]
                ):
                    agent_b_post = entry
                    break

            self.assertIsNotNone(
                agent_b_post,
                f"AgentB's disagreement '{disagreement_content}' not found on the Knowledge Board.",
            )
            if agent_b_post:
                self.assertEqual(agent_b_post["content_full"], disagreement_content)
                self.assertEqual(agent_b_post["agent_id"], self.agent_b.agent_id)
                # Simulation step for KB entry should be the step *during which* it was posted
                self.assertEqual(agent_b_post["step"], self.simulation.current_step)

            # Assertion 4: AgentB IP/DU Debit/Credit
            # If CONTINUE_COLLABORATION with a message has no specific IP cost in graph, IP should be stable.
            # DU should change based on passive generation for Analyzer role.
            du_gen_rate_b_dict = config.ROLE_DU_GENERATION.get(
                "Analyzer", {"base": 1.0, "bonus_factor": 0.2}
            )
            du_gen_rate_b = du_gen_rate_b_dict.get("base", 1.0)

            # IP should decrease by the cost per message for broadcasting disagreement
            # Corrected expectation: Agent B's action is PROPOSE_IDEA, so it incurs IP_COST_TO_POST_IDEA and gets IP_AWARD_FOR_PROPOSAL
            expected_ip_b_after_action = (
                initial_ip_b - config.IP_COST_TO_POST_IDEA + config.IP_AWARD_FOR_PROPOSAL
            )
            self.assertAlmostEqual(
                self.agent_b.state.ip,
                expected_ip_b_after_action,  # Use the corrected expected IP
                delta=0.1,
                msg=f"AgentB IP incorrect. Expected ~{expected_ip_b_after_action}, Got {self.agent_b.state.ip}",
            )

            # Expected DU: initial_du_b + passive_generation_b (no direct cost for CONTINUE_COLLABORATION in handlers)
            # Corrected: Agent B's action is PROPOSE_IDEA, so it also incurs PROPOSE_DETAILED_IDEA_DU_COST
            cost_du_propose_idea = (
                config.PROPOSE_DETAILED_IDEA_DU_COST
            )  # This was used for Agent A, should apply to B too for PROPOSE_IDEA
            min_expected_du_b = initial_du_b - cost_du_propose_idea + (0.5 * du_gen_rate_b)
            max_expected_du_b = initial_du_b - cost_du_propose_idea + (1.5 * du_gen_rate_b)

            logger.info(f"AgentB IP after turn: {self.agent_b.state.ip}")
            logger.info(
                f"AgentB DU after turn: {self.agent_b.state.du}, expected range: {min_expected_du_b:.2f} - {max_expected_du_b:.2f}"
            )

            self.assertTrue(
                min_expected_du_b
                <= self.agent_b.state.du
                <= max_expected_du_b + 0.05,  # Added epsilon
                f"AgentB DU out of expected range. Got {self.agent_b.state.du}, expected between {min_expected_du_b:.2f} and {max_expected_du_b:.2f} (+0.05 epsilon)",
            )

            # Assertion 5: AgentB's Mood and Relationship towards A
            # AgentB's mood would have been affected by *perceiving* A's message in its analyze_perception_sentiment_node.
            # The 'initial_mood_b_val' already reflects this.
            # Its own act of disagreeing (negative sentiment message) might slightly affect its own relationship *towards* A
            # if the graph logic for `finalize_message_agent_node` updates self-relationships when sending messages.
            # This is less common; usually, relationship updates happen for the *recipient*.
            # For now, we'll assert that its relationship to A hasn't drastically changed from its own action,
            # but acknowledge it might slightly shift if self-update logic exists.
            current_relationship_b_to_a = self.agent_b.state.relationships.get(
                self.agent_a.agent_id, 0.0
            )
            logger.info(
                f"AgentB's relationship towards A after its action: {current_relationship_b_to_a:.2f} (initial was {initial_relationship_b_to_a:.2f})"
            )
            # A small delta is allowed as the sentiment of its own message *could* slightly adjust its view of A.
            self.assertAlmostEqual(
                current_relationship_b_to_a,
                initial_relationship_b_to_a,
                delta=0.2,
                msg="AgentB's relationship towards A should not drastically change from its own disagreeing action.",
            )

            # AgentB's mood value already reflects perception of A's message.
            # Its own action typically doesn't cause a major immediate mood shift unless explicitly modeled.
            logger.info(
                f"AgentB mood value after its action: {self.agent_b.state.mood_value:.2f} (value after perceiving A's idea was {initial_mood_b_val:.2f})"
            )
            self.assertAlmostEqual(
                self.agent_b.state.mood_value,
                initial_mood_b_val,
                delta=0.1,
                msg="AgentB's mood should be relatively stable after its own disagreeing action, reflecting primarily the perception of A's idea.",
            )

            agent_b_turn1_thought_mock.assert_called_once()
            agent_b_turn1_intent_mock.assert_called_once()

            # Assertion 6: AgentA's state change after PERCEIVING B's disagreement (during B's turn processing)
            # MOVED THE CORE OF THIS CHECK TO THE "IMMEDIATE CHECK" SECTION ABOVE
            logger.info(
                f"AgentA's relationship towards B after B's strong disagreement (later log): {self.agent_a.state.relationships.get(self.agent_b.agent_id, 0.0):.2f} (was {relationship_a_to_b_after_own_turn:.2f})"
            )
            current_relationships_a_dict = self.agent_a.state.relationships
            value_from_direct_dict_get = current_relationships_a_dict.get(
                self.agent_b.agent_id, "KEY_NOT_FOUND_IN_DICT"
            )
            logger.info(
                f"TEST DEBUG (later log): AgentA's state id: {id(self.agent_a.state)}, relationships dict id: {id(current_relationships_a_dict)}, dict content: {current_relationships_a_dict}, value for B: {value_from_direct_dict_get}"
            )
            logger.info(
                f"TEST CHECK for agent_a towards agent_b (later log): value is {value_from_direct_dict_get}"
            )
            # The actual assertion was moved up. This is just for logging comparison if needed.
            if value_a_to_b_immediate >= relationship_a_to_b_after_own_turn:
                logger.warning(
                    "Original assertion spot would have also failed based on immediate check."
                )

            logger.info("Step 2 (AgentB posts disagreement) and assertions completed.")

        finally:
            # Restore original methods for Agent B
            self.agent_b.async_generate_role_prefixed_thought = original_agent_b_thought_gen
            self.agent_b.async_select_action_intent = original_agent_b_intent_sel

        # --- Step 3: AgentC (Facilitator) attempts to mediate ---
        logger.info(
            "Step 3: AgentC (Facilitator) perceives the situation and attempts to mediate..."
        )

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_c.agent_id,
            "Simulation did not advance to AgentC's turn.",
        )

        initial_ip_c = self.agent_c.state.ip
        initial_du_c = self.agent_c.state.du
        # Record states of A and B before C's action influences their next perception phase
        mood_a_before_c_intervention = self.agent_a.state.mood_value
        relationship_a_to_c_before_c_intervention = self.agent_a.state.relationships.get(
            self.agent_c.agent_id, 0.0
        )
        relationship_a_to_b_before_c_intervention = self.agent_a.state.relationships.get(
            self.agent_b.agent_id, 0.0
        )

        mood_b_before_c_intervention = self.agent_b.state.mood_value
        relationship_b_to_c_before_c_intervention = self.agent_b.state.relationships.get(
            self.agent_c.agent_id, 0.0
        )
        relationship_b_to_a_before_c_intervention = self.agent_b.state.relationships.get(
            self.agent_a.agent_id, 0.0
        )

        logger.info(f"AgentC initial IP for this turn: {initial_ip_c}, DU: {initial_du_c}")
        logger.info(
            f"AgentA mood before C intervention: {mood_a_before_c_intervention:.2f}, rel A->C: {relationship_a_to_c_before_c_intervention:.2f}, rel A->B: {relationship_a_to_b_before_c_intervention:.2f}"
        )
        logger.info(
            f"AgentB mood before C intervention: {mood_b_before_c_intervention:.2f}, rel B->C: {relationship_b_to_c_before_c_intervention:.2f}, rel B->A: {relationship_b_to_a_before_c_intervention:.2f}"
        )

        facilitator_intervention_message = (
            "This is a vital discussion. Perhaps we can explore both perspectives constructively? "
            "AgentA, what are the core benefits you see in radical transparency? "
            "AgentB, what are your primary concerns regarding privacy?"
        )

        # Mock AgentC's action for facilitation
        agent_c_turn1_thought_str = "The discussion between AgentA and AgentB is escalating. I should intervene to mediate and suggest finding common ground."
        agent_c_turn1_action_output = AgentActionOutput(
            thought=agent_c_turn1_thought_str,
            message_content=facilitator_intervention_message,  # This is a direct message to AgentA
            message_recipient_id=self.agent_a.agent_id,  # Facilitation directed at AgentA
            action_intent=AgentActionIntent.SEND_DIRECT_MESSAGE.value,  # Facilitator sends a DM
        )

        original_agent_c_thought_gen = self.agent_c.async_generate_role_prefixed_thought
        original_agent_c_intent_sel = self.agent_c.async_select_action_intent

        agent_c_turn1_thought_mock = AsyncMock(
            return_value=MagicMock(thought=agent_c_turn1_thought_str)
        )
        agent_c_turn1_intent_mock = AsyncMock(return_value=agent_c_turn1_action_output)
        self.agent_c.async_generate_role_prefixed_thought = agent_c_turn1_thought_mock
        self.agent_c.async_select_action_intent = agent_c_turn1_intent_mock

        try:
            await self.simulation.run_step()  # AgentC's turn

            # Check if direct message was "sent" (no direct check in this sim, but action intent implies)
            # We will check AgentC's IP/DU and if AgentA received it in perception for its next turn.
            # Assertion 7: AgentC's Action Mock Called
            agent_c_turn1_thought_mock.assert_called_once()
            agent_c_turn1_intent_mock.assert_called_once()
            logger.info(
                f"AgentC action mock called. Facilitation message to AgentA: '{facilitator_intervention_message}'"
            )

            # Assertion 8: AgentC IP/DU changes
            # Passive DU generation for Facilitator role
            du_gen_rate_c_dict = config.ROLE_DU_GENERATION.get(
                "Facilitator", {"base": 1.2, "bonus_factor": 0.3}
            )
            du_gen_rate_c = du_gen_rate_c_dict.get("base", 1.2)

            # IP should decrease by the cost per message for broadcasting intervention
            self.assertAlmostEqual(
                self.agent_c.state.ip,
                initial_ip_c - self.agent_c.state.ip_cost_per_message,
                delta=0.1,
                msg=f"AgentC IP should be decremented by ip_cost_per_message. Expected ~{initial_ip_c - self.agent_c.state.ip_cost_per_message}, Got {self.agent_c.state.ip}",
            )

            # Expected DU: initial_du_c + passive_generation_c
            min_expected_du_c = initial_du_c + (0.5 * du_gen_rate_c)
            max_expected_du_c = initial_du_c + (1.5 * du_gen_rate_c)

            logger.info(f"AgentC IP after turn: {self.agent_c.state.ip}")
            logger.info(
                f"AgentC DU after turn: {self.agent_c.state.du}, expected range: {min_expected_du_c:.2f} - {max_expected_du_c:.2f}"
            )

            self.assertTrue(
                min_expected_du_c
                <= self.agent_c.state.du
                <= max_expected_du_c + 0.05,  # Added epsilon
                f"AgentC DU out of expected range. Got {self.agent_c.state.du}, expected between {min_expected_du_c:.2f} and {max_expected_du_c:.2f} (+0.05 epsilon)",
            )

            # Assertion 9: Agent A's and B's states (mood, relationships) unchanged by C's immediate action
            # Agent A's state
            self.assertAlmostEqual(
                self.agent_a.state.mood_value,
                mood_a_before_c_intervention,
                delta=0.01,
                msg="AgentA's mood should not change until its own turn to process C's message.",
            )
            self.assertAlmostEqual(
                self.agent_a.state.relationships.get(self.agent_c.agent_id, 0.0),
                relationship_a_to_c_before_c_intervention,
                delta=0.01,
                msg="AgentA's relationship towards C should not change until A processes C's message.",
            )
            self.assertAlmostEqual(
                self.agent_a.state.relationships.get(self.agent_b.agent_id, 0.0),
                relationship_a_to_b_before_c_intervention,
                delta=0.01,
                msg="AgentA's relationship towards B should not be affected by C's broadcast at this point.",
            )

            # Agent B's state
            self.assertAlmostEqual(
                self.agent_b.state.mood_value,
                mood_b_before_c_intervention,
                delta=0.01,
                msg="AgentB's mood should not change until its own turn to process C's message.",
            )
            self.assertAlmostEqual(
                self.agent_b.state.relationships.get(self.agent_c.agent_id, 0.0),
                relationship_b_to_c_before_c_intervention,
                delta=0.01,
                msg="AgentB's relationship towards C should not change until B processes C's message.",
            )
            self.assertAlmostEqual(
                self.agent_b.state.relationships.get(self.agent_a.agent_id, 0.0),
                relationship_b_to_a_before_c_intervention,
                delta=0.01,
                msg="AgentB's relationship towards A should not be affected by C's broadcast at this point.",
            )

            logger.info("Step 3 (AgentC facilitates) and assertions completed.")

        finally:
            # Restore original methods for Agent C
            self.agent_c.async_generate_role_prefixed_thought = original_agent_c_thought_gen
            self.agent_c.async_select_action_intent = original_agent_c_intent_sel

        # --- Step 4: AgentA's second turn, processes messages ---
        logger.info("Starting Step 4: AgentA's second turn, processes messages...")

        # Verify agent turn order - should be AgentA for the start of the next round
        # self.simulation.current_step will be 3 (0-indexed for 3 agent turns)
        # self.simulation.current_agent_index will be 0 (for Agent A)
        self.assertEqual(
            self.simulation.current_step,
            3,
            "Simulation step should be 3 before Agent A's second turn.",
        )
        self.assertEqual(
            self.simulation.current_agent_index,
            0,
            "Current agent index should be 0 (Agent A) for the second round.",
        )

        # Store original Agent A methods for restoration
        original_agent_a_thought_method = self.agent_a.async_generate_role_prefixed_thought
        original_agent_a_action_method = self.agent_a.async_select_action_intent

        # Define mock output for Agent A's second turn
        agent_a_turn2_thought_str = "Agent B raised valid privacy concerns. Agent C's facilitation is helpful. I should acknowledge this and clarify my proposal's benefits while addressing privacy."
        agent_a_turn2_message_to_c_content = "Thanks, @agent_c_facilitator_conflict. I appreciate your input and @agent_b_analyzer_conflict's concerns. My aim for public logging was X, but I see the privacy angle. Can we explore a balanced approach?"

        # Log initial mood, relationships, IP and DU before Agent A's second turn
        mood_a_before_turn2 = self.agent_a.state.mood_value
        rel_a_to_b_before_turn2 = self.agent_a.state.relationships.get(self.agent_b.agent_id, 0.0)
        rel_a_to_c_before_turn2 = self.agent_a.state.relationships.get(self.agent_c.agent_id, 0.0)
        ip_a_before_turn2_action = self.agent_a.state.ip  # Define IP before turn 2 action
        du_a_before_turn2_action = self.agent_a.state.du  # Define DU before turn 2 action
        logger.info(
            f"AGENT_A_DEBUG (Before Turn 2): Mood={mood_a_before_turn2:.2f}, Rel(A->B)={rel_a_to_b_before_turn2:.2f}, Rel(A->C)={rel_a_to_c_before_turn2:.2f}, IP={ip_a_before_turn2_action}, DU={du_a_before_turn2_action}"
        )

        mock_agent_a_turn2_action_output = AgentActionOutput(
            thought=agent_a_turn2_thought_str,
            message_content=agent_a_turn2_message_to_c_content,
            message_recipient_id=self.agent_c.agent_id,  # Example: responding to the facilitator (Agent C)
            action_intent=AgentActionIntent.SEND_DIRECT_MESSAGE.value,  # Example intent
            requested_role_change=None,
            project_name_to_create=None,
            project_description_for_creation=None,
            project_id_to_join_or_leave=None,
        )

        # IMPORTANT: Ensure these mocks are applied to the correct agent instance in the simulation
        # self.simulation.agents[0] is Agent A if the order is preserved.
        current_agent_for_turn2 = self.simulation.agents[self.simulation.current_agent_index]
        self.assertEqual(
            current_agent_for_turn2.agent_id,
            self.agent_a.agent_id,
            "Mismatch: Expected Agent A for turn 2 setup.",
        )  # Add this assertion for safety

        mock_thought_method = AsyncMock(
            return_value=MagicMock(thought=mock_agent_a_turn2_action_output.thought)
        )
        mock_action_method = AsyncMock(return_value=mock_agent_a_turn2_action_output)

        current_agent_for_turn2.async_generate_role_prefixed_thought = mock_thought_method
        current_agent_for_turn2.async_select_action_intent = mock_action_method

        # Store original perceiving agent's (Agent A) state ID before its turn
        agent_a_state_id_before_turn2 = id(self.agent_a.state)
        agent_a_rels_id_before_turn2 = id(self.agent_a.state.relationships)
        logger.info(
            f"TEST CHECK (BEFORE A's TURN 2): Agent A state id: {agent_a_state_id_before_turn2}, rels id: {agent_a_rels_id_before_turn2}, rels: {self.agent_a.state.relationships}"
        )

        # AgentA's turn (processes B's disagreement from KB, C's direct message)
        # And then takes its own action (sending message to C)
        try:
            # Messages from previous turns should be moved to messages_to_perceive_this_round
            # during the start_new_round_if_needed call within run_step
            # Let's verify messages are queued for Agent A
            # Agent B's broadcast, Agent C's direct message to A
            # Filter for messages relevant to Agent A

            # Log messages about to be perceived by Agent A. These are from self.simulation.pending_messages_for_next_round
            # which becomes self.simulation.messages_to_perceive_this_round at the start of run_step
            logger.info(
                f"AgentA (Turn 2) is about to perceive messages from self.simulation.pending_messages_for_next_round: {self.simulation.pending_messages_for_next_round}"
            )
            # This check is before run_step, where pending becomes current.
            # After run_step, `pending_messages_for_next_round` will contain messages generated *by Agent A* in this turn.
            # `messages_to_perceive_this_round` *inside run_step* will have B's and C's messages.

            expected_messages_for_a_in_perception = [
                msg
                for msg in self.simulation.pending_messages_for_next_round  # These are about to be moved
                if msg["recipient_id"] == self.agent_a.agent_id or msg["recipient_id"] is None
            ]
            logger.info(
                f"Messages pending for Agent A to perceive (before this run_step): {expected_messages_for_a_in_perception}"
            )
            # self.assertTrue(len(expected_messages_for_a_in_perception) >= 2, "Agent A should have at least 2 messages to perceive (from B and C).")

            await self.simulation.run_step()  # AgentA's second turn (now Step 1 of Round 1)

            # Assert that Agent A's methods were called
            mock_thought_method.assert_called_once()
            mock_action_method.assert_called_once()

            # Log final mood and relationships after Agent A's second turn
            mood_a_after_turn2 = self.agent_a.state.mood_value
            rel_a_to_b_after_turn2 = self.agent_a.state.relationships.get(
                self.agent_b.agent_id, 0.0
            )
            rel_a_to_c_after_turn2 = self.agent_a.state.relationships.get(
                self.agent_c.agent_id, 0.0
            )
            logger.info(
                f"AGENT_A_DEBUG (After Turn 2): Mood={mood_a_after_turn2:.2f}, Rel(A->B)={rel_a_to_b_after_turn2:.2f}, Rel(A->C)={rel_a_to_c_after_turn2:.2f}"
            )

            # Assertion 1: AgentA's mood should change due to negative message from B and neutral/positive from C
            self.assertNotAlmostEqual(
                mood_a_after_turn2,
                mood_a_before_turn2,
                delta=0.001,
                msg="AgentA's mood should have changed after processing B's disagreement and C's facilitation.",
            )

            # Assertion 2: AgentA's relationship towards AgentB should worsen
            self.assertLess(
                rel_a_to_b_after_turn2,
                rel_a_to_b_before_turn2,
                "AgentA's relationship towards B should have worsened after B's disagreement.",
            )
            self.assertAlmostEqual(
                rel_a_to_b_after_turn2,
                -0.28,
                delta=0.01,  # Based on -0.7 sentiment, 0.4 neg_lr
                msg="AgentA->B relationship score is not as expected.",
            )

            # Assertion 3: AgentA's relationship towards AgentC might improve or stay neutral
            self.assertGreater(
                rel_a_to_c_after_turn2,
                rel_a_to_c_before_turn2 - 0.001,  # allow for neutral if sentiment was 0
                "AgentA's relationship towards C should have improved or stayed neutral.",
            )
            self.assertAlmostEqual(
                rel_a_to_c_after_turn2,
                0.18,
                delta=0.01,  # Based on 0.2 sentiment, 0.3 pos_lr, targeted=True
                msg="AgentA->C relationship score is not as expected.",
            )

            # Assertion 4: AgentA IP/DU after its own action (sending DM to C)
            cost_ip_send_dm = config.IP_COST_SEND_DIRECT_MESSAGE
            # No specific DU cost for sending DM in config, assume 0 for now unless specified in action node
            du_cost_send_dm = (
                0  # Assuming basic_agent_graph.handle_send_direct_message_node has no DU cost.
            )

            # Passive DU generation for Innovator role (again for this new step)
            du_gen_rate_a_dict_t2 = config.ROLE_DU_GENERATION.get(
                "Innovator", {"base": 1.0, "bonus_factor": 0.5}
            )
            du_gen_rate_a_t2 = du_gen_rate_a_dict_t2.get("base", 1.0)

            # IP after perceive (no change), then -cost_ip_send_dm for its own action
            # DU after perceive (no change), then -du_cost_send_dm + passive_generation for its own action
            # initial_ip_a_turn2 and initial_du_a_turn2 were before perception.
            # Mood/Rel updates happen during perception. IP/DU for own action happens after.

            # Let's get IP/DU *after perception effects* but *before own action costs*
            # This is tricky because perception and action are bundled in run_step -> agent.run_turn -> graph
            # The IP/DU at self.agent_a.state.ip now is *after* its own action.

            expected_ip_a_after_turn2_action = ip_a_before_turn2_action - cost_ip_send_dm
            # DU is more complex: initial_du_a_turn2 (before this turn) - du_cost_send_dm + passive_gen_this_turn
            min_expected_du_a_turn2_action = (
                du_a_before_turn2_action - du_cost_send_dm + (0.5 * du_gen_rate_a_t2)
            )
            max_expected_du_a_turn2_action = (
                du_a_before_turn2_action - du_cost_send_dm + (1.5 * du_gen_rate_a_t2)
            )

            logger.info(
                f"AgentA IP after its second turn action: {self.agent_a.state.ip}, expected: {expected_ip_a_after_turn2_action}"
            )
            logger.info(
                f"AgentA DU after its second turn action: {self.agent_a.state.du}, expected range: {min_expected_du_a_turn2_action:.2f} - {max_expected_du_a_turn2_action:.2f}"
            )

            self.assertAlmostEqual(
                self.agent_a.state.ip,
                expected_ip_a_after_turn2_action,
                delta=0.01,
                msg="AgentA IP after second turn action incorrect.",
            )
            self.assertTrue(
                min_expected_du_a_turn2_action
                <= self.agent_a.state.du
                <= max_expected_du_a_turn2_action + 0.05,
                "AgentA DU after second turn action out of expected range.",
            )

            # Assertion 5: Agent A's message to Agent C should be in pending_messages
            # (or messages_to_perceive_this_round if sim has advanced to next round perception phase already)
            # For now, check pending_messages_for_next_round as current_step was just completed for Agent A
            # The current_step would have been incremented by run_step to 4.
            # Messages sent in step 3 (Agent A's turn) will be recorded with step=3.

            # Correct step for Agent A's second message is self.simulation.current_step -1 (because current_step was already incremented)
            # However, the mock is for its turn, so the message in pending should reflect the step it was *generated* in.
            # When agent A (index 0) runs, simulation current_step is 3.
            # After its turn, simulation.current_step becomes 4.
            # Messages are logged with the step they occurred in.
            # Agent A's turn was step 3 of the simulation (0-indexed overall steps).

            agent_a_message_to_c_found = False
            for (
                msg
            ) in (
                self.simulation.pending_messages_for_next_round
            ):  # Check messages generated THIS turn
                if (
                    msg["recipient_id"] == self.agent_c.agent_id
                    and msg["sender_id"] == self.agent_a.agent_id
                    and msg["content"] == agent_a_turn2_message_to_c_content
                ):
                    agent_a_message_to_c_found = True
                    break
            self.assertTrue(
                agent_a_message_to_c_found,
                "Agent A's message to Agent C (from this turn) should be in pending_messages",
            )

            logger.info("Step 4 (AgentA processes messages) and assertions completed.")

        finally:
            # Restore original methods for Agent A
            current_agent_for_turn2.async_generate_role_prefixed_thought = (
                original_agent_a_thought_method
            )
            current_agent_for_turn2.async_select_action_intent = original_agent_a_action_method

        # --- Step 5: Agent B's Second Turn - Processes Agent C's Facilitation ---
        logger.info("Starting Step 5: AgentB's second turn, processes C's facilitation...")

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_b.agent_id,
            "Simulation did not advance to AgentB's second turn.",
        )

        # Record Agent B's state *before* this turn's processing
        mood_b_before_processing_c_msg = self.agent_b.state.mood_value
        relationship_b_to_a_before_processing_c_msg = self.agent_b.state.relationships.get(
            self.agent_a.agent_id, 0.0
        )
        relationship_b_to_c_before_processing_c_msg = self.agent_b.state.relationships.get(
            self.agent_c.agent_id, 0.0
        )
        initial_ip_b_turn2 = self.agent_b.state.ip
        initial_du_b_turn2 = self.agent_b.state.du

        logger.info(
            f"AgentB MoodValue before processing C's facilitation: {mood_b_before_processing_c_msg:.2f}"
        )
        logger.info(
            f"AgentB->A Relationship before processing C's facilitation: {relationship_b_to_a_before_processing_c_msg:.2f}"
        )
        logger.info(
            f"AgentB->C Relationship before processing C's facilitation: {relationship_b_to_c_before_processing_c_msg:.2f}"
        )
        logger.info(f"AgentB IP before this turn: {initial_ip_b_turn2}, DU: {initial_du_b_turn2}")

        # Allow Agent B's graph to determine its action.
        await self.simulation.run_step()  # AgentB's second turn

        # Assertion 13: Agent B's Mood Update
        mood_b_after_processing_c_msg = self.agent_b.state.mood_value
        logger.info(
            f"AgentB MoodValue after processing C's facilitation: {mood_b_after_processing_c_msg:.2f}"
        )
        # Agent C's facilitation message was positive/neutral.
        # Expect mood to improve or stay stable, allowing for slight decay or minor negative if B's action was costly.
        self.assertGreaterEqual(
            mood_b_after_processing_c_msg,
            mood_b_before_processing_c_msg - 0.1,
            msg="AgentB's mood should improve or stay relatively stable after C's positive/neutral facilitation.",
        )

        # Assertion 14: Agent B's Relationship Updates
        relationship_b_to_a_after_processing_c_msg = self.agent_b.state.relationships.get(
            self.agent_a.agent_id, 0.0
        )
        relationship_b_to_c_after_processing_c_msg = self.agent_b.state.relationships.get(
            self.agent_c.agent_id, 0.0
        )

        logger.info(
            f"AgentB->A Relationship after processing C's facilitation: {relationship_b_to_a_after_processing_c_msg:.2f}"
        )
        logger.info(
            f"AgentB->C Relationship after processing C's facilitation: {relationship_b_to_c_after_processing_c_msg:.2f}"
        )

        # B's relationship with A might not change much just from C's message about their dynamic.
        # The main impact was when B sent the disagreement.
        self.assertAlmostEqual(
            relationship_b_to_a_after_processing_c_msg,
            relationship_b_to_a_before_processing_c_msg,
            delta=0.1,
            msg="AgentB's relationship with AgentA should be relatively stable this turn.",
        )

        # B perceived C's facilitation. Relationship B->C should improve or stay stable.
        self.assertGreaterEqual(
            relationship_b_to_c_after_processing_c_msg,
            relationship_b_to_c_before_processing_c_msg,
            msg="AgentB's relationship with AgentC should improve or stay stable after C's facilitation.",
        )

        # Assertion 15: Agent B IP/DU Debit
        final_ip_b_turn2 = self.agent_b.state.ip
        final_du_b_turn2 = self.agent_b.state.du
        logger.info(f"AgentB IP after this turn: {final_ip_b_turn2}, DU: {final_du_b_turn2}")

        if self.agent_b.state.last_action_intent != AgentActionIntent.IDLE.value:
            self.assertTrue(
                final_du_b_turn2 > initial_du_b_turn2 - 5.0,
                f"AgentB DU changed unexpectedly. From {initial_du_b_turn2} to {final_du_b_turn2}",
            )
        else:  # Idle
            expected_du_b_turn2 = initial_du_b_turn2
            self.assertAlmostEqual(
                final_du_b_turn2,
                expected_du_b_turn2,
                delta=0.1,
                msg=f"AgentB DU should be stable if idle. From {initial_du_b_turn2} to {final_du_b_turn2}",
            )

        logger.info("Step 5 (AgentB processes C's facilitation) and assertions completed.")

        # --- Step 5.5: Agent C's Second Turn - Processes Agent A's response ---
        logger.info("Starting Step 5.5: AgentC's second turn, processes Agent A's response...")

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_c.agent_id,
            "Simulation did not advance to AgentC's second turn.",
        )

        mood_c_before_turn2 = self.agent_c.state.mood_value
        rel_c_to_a_before_turn2 = self.agent_c.state.relationships.get(self.agent_a.agent_id, 0.0)
        rel_c_to_b_before_turn2 = self.agent_c.state.relationships.get(self.agent_b.agent_id, 0.0)
        ip_c_before_turn2 = self.agent_c.state.ip
        du_c_before_turn2 = self.agent_c.state.du
        logger.info(
            f"AgentC (Turn 2 PRE): Mood={mood_c_before_turn2:.2f}, Rel(C->A)={rel_c_to_a_before_turn2:.2f}, Rel(C->B)={rel_c_to_b_before_turn2:.2f}, IP={ip_c_before_turn2}, DU={du_c_before_turn2}"
        )

        agent_c_turn2_thought_str = "Agent A responded positively to my facilitation and is open to exploring a balanced approach with Agent B. This is good progress. I should encourage Agent A to direct their question to Agent B."
        agent_c_turn2_message_content = "That's a constructive step, @agent_a_innovator_conflict. Perhaps directing your question about safeguards to @agent_b_analyzer_conflict would be helpful now?"

        mock_agent_c_turn2_action_output = AgentActionOutput(
            thought=agent_c_turn2_thought_str,
            message_content=agent_c_turn2_message_content,
            message_recipient_id=self.agent_a.agent_id,
            action_intent=AgentActionIntent.SEND_DIRECT_MESSAGE.value,
            requested_role_change=None,
        )

        # Store original Agent C methods
        original_agent_c_thought_method = self.agent_c.async_generate_role_prefixed_thought
        original_agent_c_action_method = self.agent_c.async_select_action_intent

        try:
            self.agent_c.async_generate_role_prefixed_thought = AsyncMock(
                return_value=dspy.Prediction(thought=agent_c_turn2_thought_str)
            )
            self.agent_c.async_select_action_intent = AsyncMock(
                return_value=mock_agent_c_turn2_action_output
            )

            await self.simulation.run_step()  # Agent C's second turn

            self.agent_c.async_generate_role_prefixed_thought.assert_called_once()
            self.agent_c.async_select_action_intent.assert_called_once()
        finally:
            # Restore original methods for Agent C
            self.agent_c.async_generate_role_prefixed_thought = original_agent_c_thought_method
            self.agent_c.async_select_action_intent = original_agent_c_action_method

        # Assertions for Agent C's state
        mood_c_after_turn2 = self.agent_c.state.mood_value
        rel_c_to_a_after_turn2 = self.agent_c.state.relationships.get(self.agent_a.agent_id, 0.0)
        ip_c_after_turn2 = self.agent_c.state.ip
        du_c_after_turn2 = self.agent_c.state.du
        # Assuming du_gen_rate_c_t2 uses the base rate from config if specific role entry is missing
        du_gen_rate_c_t2_config = config.ROLE_DU_GENERATION.get(
            self.agent_c.state.role, {"base": 1.0}
        )
        du_gen_rate_c_t2 = du_gen_rate_c_t2_config.get("base", 1.0)

        cost_ip_send_dm_c = getattr(config, "IP_COST_TARGETED_MESSAGE", 1.0)

        logger.info(
            f"AgentC (Turn 2 POST): Mood={mood_c_after_turn2:.2f}, Rel(C->A)={rel_c_to_a_after_turn2:.2f}, IP={ip_c_after_turn2}, DU={du_c_after_turn2}"
        )

        self.assertGreaterEqual(
            mood_c_after_turn2,
            mood_c_before_turn2 - 0.05,
            "AgentC's mood should remain stable or improve slightly.",
        )
        self.assertGreaterEqual(
            rel_c_to_a_after_turn2,
            rel_c_to_a_before_turn2 - 0.05,
            "AgentC's relationship with AgentA should remain positive/stable.",
        )

        expected_ip_c_t2 = ip_c_before_turn2 - cost_ip_send_dm_c
        self.assertAlmostEqual(
            ip_c_after_turn2,
            expected_ip_c_t2,
            delta=0.1,
            msg=f"AgentC IP incorrect. Got {ip_c_after_turn2}, expected {expected_ip_c_t2}",
        )

        min_expected_du_c_t2 = du_c_before_turn2 + (0.5 * du_gen_rate_c_t2)
        max_expected_du_c_t2 = du_c_before_turn2 + (1.5 * du_gen_rate_c_t2)
        self.assertTrue(
            min_expected_du_c_t2 <= du_c_after_turn2 <= max_expected_du_c_t2,
            f"AgentC DU out of range. Got {du_c_after_turn2}, expected {min_expected_du_c_t2:.2f}-{max_expected_du_c_t2:.2f}",
        )

        agent_c_message_to_a_found_t2 = False
        for msg in self.simulation.pending_messages_for_next_round:
            if (
                msg["sender_id"] == self.agent_c.agent_id
                and msg["recipient_id"] == self.agent_a.agent_id
                and msg["content"] == agent_c_turn2_message_content
            ):
                agent_c_message_to_a_found_t2 = True
                break
        self.assertTrue(
            agent_c_message_to_a_found_t2,
            "Agent C's message to Agent A (from this turn) should be in pending_messages",
        )

        logger.info("Step 5.5 (AgentC's second turn) and assertions completed.")

        # --- Step 6: Agent A's Third Turn - Responds More Constructively ---
        # Renamed from Step 6 to Step 7, but keeping the "original" step number in log for now
        logger.info(
            "Starting Step 6 (Original): AgentA's third turn, responding constructively..."
        )

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_a.agent_id,
            "Simulation did not advance to AgentA's third turn (after C's second turn).",
        )

        # Record Agent A's state *before* this turn's action
        mood_a_before_third_action = self.agent_a.state.mood_value
        relationship_a_to_b_before_third_action = self.agent_a.state.relationships.get(
            self.agent_b.agent_id, 0.0
        )
        relationship_a_to_c_before_third_action = self.agent_a.state.relationships.get(
            self.agent_c.agent_id, 0.0
        )
        initial_ip_a_turn3 = self.agent_a.state.ip
        initial_du_a_turn3 = self.agent_a.state.du

        logger.info(f"AgentA MoodValue before third action: {mood_a_before_third_action:.2f}")
        logger.info(
            f"AgentA->B Relationship before third action: {relationship_a_to_b_before_third_action:.2f}"
        )
        logger.info(
            f"AgentA->C Relationship before third action: {relationship_a_to_c_before_third_action:.2f}"
        )
        logger.info(f"AgentA IP before this turn: {initial_ip_a_turn3}, DU: {initial_du_a_turn3}")

        conciliatory_message_a = (
            "Thank you, AgentC_Conflict, for the perspective. AgentB_Conflict, I understand your concerns about privacy. "
            "Could you elaborate on specific safeguards you'd find acceptable if some transparency is pursued?"
        )

        mock_agent_a_third_action_output = AgentActionOutput(
            thought="I should try to de-escalate and understand AgentB's concerns better, as AgentC suggested.",
            message_content=conciliatory_message_a,
            message_recipient_id=self.agent_b.agent_id,  # Direct question to AgentB
            action_intent=AgentActionIntent.ASK_CLARIFICATION.value,
            requested_role_change=None,
            project_name_to_create=None,
            project_description_for_creation=None,
            project_id_to_join_or_leave=None,
        )

        with patch(
            "src.agents.graphs.basic_agent_graph.generate_structured_output",
            return_value=mock_agent_a_third_action_output,
        ) as mock_gen_struct_output_a_turn3:
            await self.simulation.run_step()  # AgentA's third turn

            # Assertion 16: AgentA's Message Sent
            mock_gen_struct_output_a_turn3.assert_called_once()
            logger.info(f"AgentA (Turn 3) action mock called. Message: '{conciliatory_message_a}'")

            # Assertion 17: AgentA's Mood Update
            mood_a_after_third_action = self.agent_a.state.mood_value
            logger.info(f"AgentA MoodValue after third action: {mood_a_after_third_action:.2f}")
            # Taking a constructive stance. Mood might stabilize or slightly improve from its previous low.
            # It should ideally be better than mood_a_after_processing_msgs (after B's direct hit and C's general facilitation)
            self.assertGreaterEqual(
                mood_a_after_third_action,
                mood_a_after_turn2 - 0.05,  # Allow very slight dip if action has minor cost aspect
                "AgentA's mood should stabilize or improve after taking a constructive action.",
            )

            # Assertion 18: AgentA's Relationship Updates
            relationship_a_to_b_after_third_action = self.agent_a.state.relationships.get(
                self.agent_b.agent_id, 0.0
            )
            relationship_a_to_c_after_third_action = self.agent_a.state.relationships.get(
                self.agent_c.agent_id, 0.0
            )

            logger.info(
                f"AgentA->B Relationship after third action: {relationship_a_to_b_after_third_action:.2f}"
            )
            logger.info(
                f"AgentA->C Relationship after third action: {relationship_a_to_c_after_third_action:.2f}"
            )

            # A asked B a constructive question. Relationship A->B should improve or at least not worsen further.
            # The graph logic updates relationship when sending a message.
            self.assertGreater(
                relationship_a_to_b_after_third_action,
                relationship_a_to_b_before_third_action
                - 0.05,  # Allow for tiny negative if sentiment of message is not purely positive
                "AgentA's relationship with AgentB should start to improve or stabilize.",
            )

            # Relationship with C might not change significantly from this specific action towards B,
            # but should remain positive due to C's earlier facilitation.
            self.assertGreaterEqual(
                relationship_a_to_c_after_third_action,
                relationship_a_to_c_before_third_action - 0.05,  # Allow for slight decay
                "AgentA's relationship with AgentC should remain stable or positive.",
            )

            # Assertion 19: AgentA IP/DU Debit
            final_ip_a_turn3 = self.agent_a.state.ip
            final_du_a_turn3 = self.agent_a.state.du
            du_gen_rate_a_turn3 = config.ROLE_DU_GENERATION.get(self.agent_a.state.role, 1.0)
            cost_du_ask_clarification = config.DU_COST_REQUEST_DETAILED_CLARIFICATION

            logger.info(f"AgentA IP after third turn: {final_ip_a_turn3}, DU: {final_du_a_turn3}")

            # IP should be stable for ASK_CLARIFICATION unless graph has specific costs.
            self.assertAlmostEqual(
                final_ip_a_turn3,
                initial_ip_a_turn3,
                delta=0.1,
                msg=f"AgentA IP should be stable. From {initial_ip_a_turn3} to {final_ip_a_turn3}",
            )

            # DU cost for asking clarification + passive generation.
            # Expected DU: initial_du_a_turn3 - cost_du_ask_clarification + passive_generation
            min_expected_du_a_turn3 = (
                initial_du_a_turn3 - cost_du_ask_clarification + (0.5 * du_gen_rate_a_turn3)
            )
            max_expected_du_a_turn3 = (
                initial_du_a_turn3 - cost_du_ask_clarification + (1.5 * du_gen_rate_a_turn3)
            )

            self.assertTrue(
                min_expected_du_a_turn3 <= final_du_a_turn3 <= max_expected_du_a_turn3,
                f"AgentA DU out of expected range. Got {final_du_a_turn3}, expected {min_expected_du_a_turn3:.2f}-{max_expected_du_a_turn3:.2f}",
            )

        logger.info("Step 6 (Original) (AgentA responds constructively) and assertions completed.")

        # --- Step 7: Agent B's Third Turn - Responds to A's Conciliatory Move ---
        # Renamed from Step 7 to Step 8
        logger.info(
            "Starting Step 7 (Original): AgentB's third turn, responding constructively to A..."
        )

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_b.agent_id,
            "Simulation did not advance to AgentB's third turn (after A's third turn).",
        )

        # Record Agent B's state *before* this turn's action
        mood_b_before_third_action = self.agent_b.state.mood_value
        relationship_b_to_a_before_third_action = self.agent_b.state.relationships.get(
            self.agent_a.agent_id, 0.0
        )
        relationship_b_to_c_before_third_action = self.agent_b.state.relationships.get(
            self.agent_c.agent_id, 0.0
        )
        initial_ip_b_turn3 = self.agent_b.state.ip
        initial_du_b_turn3 = self.agent_b.state.du

        logger.info(f"AgentB MoodValue before third action: {mood_b_before_third_action:.2f}")
        logger.info(
            f"AgentB->A Relationship before third action: {relationship_b_to_a_before_third_action:.2f}"
        )
        logger.info(
            f"AgentB->C Relationship before third action: {relationship_b_to_c_before_third_action:.2f}"
        )
        logger.info(f"AgentB IP before this turn: {initial_ip_b_turn3}, DU: {initial_du_b_turn3}")

        constructive_response_b = (
            "Thank you for asking, AgentA_Innovator_Conflict. My main concern is the potential for misuse of sensitive strategic information if all comms are public. "
            "However, I'm open to discussing solutions that balance transparency with necessary safeguards, perhaps for specific project discussions?"
        )

        mock_agent_b_third_action_output = AgentActionOutput(
            thought="AgentA is being more reasonable. I should respond constructively and answer their question.",
            message_content=constructive_response_b,
            message_recipient_id=self.agent_a.agent_id,  # Direct response to AgentA
            action_intent=AgentActionIntent.SEND_DIRECT_MESSAGE.value,
            requested_role_change=None,
            project_name_to_create=None,
            project_description_for_creation=None,
            project_id_to_join_or_leave=None,
        )

        with patch(
            "src.agents.graphs.basic_agent_graph.generate_structured_output",
            return_value=mock_agent_b_third_action_output,
        ) as mock_gen_struct_output_b_turn3:
            await self.simulation.run_step()  # AgentB's third turn

            # Assertion 20: AgentB's Message Sent
            mock_gen_struct_output_b_turn3.assert_called_once()
            logger.info(
                f"AgentB (Turn 3) action mock called. Message: '{constructive_response_b}'"
            )

            # Assertion 21: AgentB's Mood Update
            mood_b_after_third_action = self.agent_b.state.mood_value
            logger.info(f"AgentB MoodValue after third action: {mood_b_after_third_action:.2f}")
            # Responding constructively to A's positive move. Mood should improve or remain stable and positive.
            # It was already somewhat stable after C's facilitation, A's direct positive outreach should help.
            self.assertGreaterEqual(
                mood_b_after_third_action,
                mood_b_before_third_action - 0.05,  # Allow for slight dip due to action itself
                "AgentB's mood should remain stable or improve after A's constructive outreach and B's constructive response.",
            )

            # Assertion 22: AgentB's Relationship Updates
            relationship_b_to_a_after_third_action = self.agent_b.state.relationships.get(
                self.agent_a.agent_id, 0.0
            )
            relationship_b_to_c_after_third_action = self.agent_b.state.relationships.get(
                self.agent_c.agent_id, 0.0
            )

            logger.info(
                f"AgentB->A Relationship after third action: {relationship_b_to_a_after_third_action:.2f}"
            )
            logger.info(
                f"AgentB->C Relationship after third action: {relationship_b_to_c_after_third_action:.2f}"
            )

            # B responded constructively to A. Relationship B->A should improve.
            self.assertGreater(
                relationship_b_to_a_after_third_action,
                relationship_b_to_a_before_third_action,
                "AgentB's relationship with AgentA should improve after constructive exchange.",
            )

            # Relationship with C should remain stable/positive.
            self.assertGreaterEqual(
                relationship_b_to_c_after_third_action,
                relationship_b_to_c_before_third_action - 0.05,  # Allow for slight decay
                "AgentB's relationship with AgentC should improve or stay stable after C's facilitation.",
            )

        # Assertion 23: Agent B IP/DU Debit
        final_ip_b_turn3 = self.agent_b.state.ip
        final_du_b_turn3 = self.agent_b.state.du
        logger.info(f"AgentB IP after third turn: {final_ip_b_turn3}, DU: {final_du_b_turn3}")

        if self.agent_b.state.last_action_intent != AgentActionIntent.IDLE.value:
            self.assertTrue(
                final_du_b_turn3 > initial_du_b_turn3 - 5.0,
                f"AgentB DU changed unexpectedly. From {initial_du_b_turn3} to {final_du_b_turn3}",
            )
        else:  # Idle
            expected_du_b_turn3 = initial_du_b_turn3
            self.assertAlmostEqual(
                final_du_b_turn3,
                expected_du_b_turn3,
                delta=0.1,
                msg=f"AgentB DU should be stable if idle. From {initial_du_b_turn3} to {final_du_b_turn3}",
            )

        logger.info("Step 7 (AgentB responds constructively to A) and assertions completed.")

        # --- Step 8: Agent C's Second Turn - Concluding/Summarizing Broadcast ---
        logger.info("Starting Step 8: AgentC's second turn, concluding summary...")

        self.assertEqual(
            self.simulation.agents[self.simulation.current_agent_index].agent_id,
            self.agent_c.agent_id,
            "Simulation did not advance to AgentC's second turn.",
        )

        initial_ip_c_turn2 = self.agent_c.state.ip
        initial_du_c_turn2 = self.agent_c.state.du
        logger.info(
            f"AgentC initial IP for second turn: {initial_ip_c_turn2}, DU: {initial_du_c_turn2}"
        )

        resolution_summary_c = (
            "It's great to see such productive dialogue emerging! It seems we've found a good path "
            "forward by understanding each other's core points. Well done, AgentA_Innovator_Conflict and AgentB_Analyzer_Conflict."
        )

        mock_agent_c_second_action_output = AgentActionOutput(
            thought="The conflict appears to be resolving. I should acknowledge the positive progress and summarize.",
            message_content=resolution_summary_c,
            message_recipient_id=None,  # Broadcast
            action_intent=AgentActionIntent.CONTINUE_COLLABORATION.value,
            requested_role_change=None,
            project_name_to_create=None,
            project_description_for_creation=None,
            project_id_to_join_or_leave=None,
        )

        with patch(
            "src.agents.graphs.basic_agent_graph.generate_structured_output",
            return_value=mock_agent_c_second_action_output,
        ) as mock_gen_struct_output_c_turn2:
            await self.simulation.run_step()  # AgentC's second turn

            # Assertion 24: AgentC's Message Sent
            mock_gen_struct_output_c_turn2.assert_called_once()
            logger.info(f"AgentC (Turn 2) action mock called. Message: '{resolution_summary_c}'")

            # Assertion 25: AgentC IP/DU Debit
            final_ip_c_turn2 = self.agent_c.state.ip
            final_du_c_turn2 = self.agent_c.state.du
            du_gen_rate_c_turn2 = config.ROLE_DU_GENERATION.get(self.agent_c.state.role, 1.0)

            logger.info(f"AgentC IP after second turn: {final_ip_c_turn2}, DU: {final_du_c_turn2}")

            # IP should be stable for CONTINUE_COLLABORATION.
            self.assertAlmostEqual(
                final_ip_c_turn2,
                initial_ip_c_turn2,
                delta=0.1,
                msg=f"AgentC IP should be stable. From {initial_ip_c_turn2} to {final_ip_c_turn2}",
            )

            # DU should increase due to passive generation.
            min_expected_du_c_turn2 = initial_du_c_turn2 + (0.5 * du_gen_rate_c_turn2)
            max_expected_du_c_turn2 = initial_du_c_turn2 + (1.5 * du_gen_rate_c_turn2)

            self.assertTrue(
                min_expected_du_c_turn2 <= final_du_c_turn2 <= max_expected_du_c_turn2,
                f"AgentC DU out of expected range. Got {final_du_c_turn2}, expected {min_expected_du_c_turn2:.2f}-{max_expected_du_c_turn2:.2f}",
            )

        logger.info("Step 8 (AgentC summarizes resolution) and assertions completed.")

        # --- Final Overall State Assertions (Post-Resolution) ---
        logger.info("Performing Final Overall State Assertions...")

        # Assertion 26: Agent A's Final State
        logger.info(
            f"AgentA final mood: {self.agent_a.state.mood} (value: {self.agent_a.state.mood_value:.2f})"
        )
        logger.info(
            f"AgentA final rel A->B: {self.agent_a.state.relationships.get(self.agent_b.agent_id, 0.0):.2f}"
        )
        logger.info(
            f"AgentA final rel A->C: {self.agent_a.state.relationships.get(self.agent_c.agent_id, 0.0):.2f}"
        )

        self.assertGreaterEqual(
            self.agent_a.state.mood_value,
            -0.1,  # Neutral or slightly positive
            "AgentA's final mood should be neutral or positive after resolution.",
        )
        # Relationship A->B started at 0, dipped, then recovered.
        # It was `relationship_a_to_b_after_third_action` in Step 6. Let's compare to that.
        # It should be better than the lowest point (e.g., relationship_a_to_b_after_processing_msgs)
        # and ideally close to or better than relationship_a_to_b_before_third_action or even initial 0.0
        self.assertGreaterEqual(
            self.agent_a.state.relationships.get(self.agent_b.agent_id, 0.0),
            relationship_a_to_b_before_third_action
            - 0.1,  # Should be at least as good as before its conciliatory move
            "AgentA's final relationship with AgentB should be significantly improved.",
        )
        self.assertGreater(
            self.agent_a.state.relationships.get(self.agent_c.agent_id, 0.0),
            0.0,
            "AgentA's final relationship with AgentC should be positive.",
        )

        # Assertion 27: Agent B's Final State
        logger.info(
            f"AgentB final mood: {self.agent_b.state.mood} (value: {self.agent_b.state.mood_value:.2f})"
        )
        logger.info(
            f"AgentB final rel B->A: {self.agent_b.state.relationships.get(self.agent_a.agent_id, 0.0):.2f}"
        )
        logger.info(
            f"AgentB final rel B->C: {self.agent_b.state.relationships.get(self.agent_c.agent_id, 0.0):.2f}"
        )

        self.assertGreaterEqual(
            self.agent_b.state.mood_value,
            -0.1,  # Neutral or slightly positive
            "AgentB's final mood should be neutral or positive after resolution.",
        )
        # Relationship B->A started at 0, dipped, then recovered.
        # It was `relationship_b_to_a_after_third_action` in Step 7.
        self.assertGreaterEqual(
            self.agent_b.state.relationships.get(self.agent_a.agent_id, 0.0),
            relationship_b_to_a_before_third_action,  # Should be at least as good as before its constructive response
            "AgentB's final relationship with AgentA should be significantly improved.",
        )
        self.assertGreater(
            self.agent_b.state.relationships.get(self.agent_c.agent_id, 0.0),
            0.0,
            "AgentB's final relationship with AgentC should be positive.",
        )

        # Assertion 28: Knowledge Board Content (Optional)
        # Check for the initial controversial post by AgentA and the strong disagreement by AgentB.
        # Other messages (facilitation, conciliatory question, constructive response) might have been direct or broadcast
        # and not necessarily on the KB depending on AgentActionIntent.
        board_entries = self.simulation.knowledge_board.get_full_entries()
        logger.info(f"Final Knowledge Board Entries: {board_entries}")

        found_a_initial_post = any(
            entry["agent_id"] == self.agent_a.agent_id
            and controversial_idea_content in entry["content_full"]
            for entry in board_entries
        )
        # Disagreement by B was a broadcast, so should appear as a message perceived by others,
        # but not necessarily a distinct KB "entry" unless CONTINUE_COLLABORATION with content is treated as such by the sim.
        # The current basic_agent_graph `handle_propose_idea_node` adds to KB.
        # `handle_continue_collaboration_node` does not explicitly add to KB.
        # So we'll only check for A's initial post.
        self.assertTrue(
            found_a_initial_post,
            "AgentA's initial controversial idea should be on the Knowledge Board.",
        )

        # Example of checking B's disagreement if it were a KB post:
        # found_b_disagreement_post = any(
        #    entry["agent_id"] == self.agent_b.agent_id and disagreement_content in entry["content"]
        #    for entry in board_entries
        # )
        # self.assertTrue(found_b_disagreement_post, "AgentB's disagreement should be on the Knowledge Board if posted as an idea.")

        logger.info("All conflict resolution steps and final assertions completed.")


# To run this test specifically (requires pytest-asyncio):
# python -m pytest -m integration tests/integration/conflict_resolution/test_conflict_resolution_scenario.py
if __name__ == "__main__":
    # This allows running the test file directly for easier debugging,
    # though pytest is preferred for full test suite runs.
    unittest.main()
